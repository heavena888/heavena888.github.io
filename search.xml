<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>cnn</title>
    <url>/2020/08/10/CNN/</url>
    <content><![CDATA[<h1 id="cnn"><a href="#cnn" class="headerlink" title="cnn"></a>cnn</h1><h2 id="cnn重要的步骤"><a href="#cnn重要的步骤" class="headerlink" title="cnn重要的步骤"></a>cnn重要的步骤</h2><p>Convolution(卷积)，Max Pooling(池化)， Fully Connected Feedforward network(全连接层)<br><a id="more"></a><br>大致步骤，传入一张图，通过卷积-&gt;池化-&gt;卷积-&gt;池化等反复训练，最后通过全连接层将获得的特征展开，进行DNN，最后ont-hot得到一个概率较高的东西，最后得到图像为什么物品。</p>
<h2 id="Convolution-卷积"><a href="#Convolution-卷积" class="headerlink" title="Convolution(卷积)"></a>Convolution(卷积)</h2><p>卷积主要有Filter(过滤器)，stride(步数)，feature map(特征图)</p>
<p><img src="https://gitee.com/heavena888/photo/raw/master/img/QQ图片20200812224841.png" alt=""></p>
<p>feature map，从输入的图片开始，以mnist为列，如果是灰度图则为28 $\times$ 28 $\times$ 1，如果为RGB则为28 $\times$ 28 $\times$ 3，然后开始第一轮的卷积，以图为例，stride为1，在featrue里面寻找与filter同样大小的矩阵，将feature里面的矩阵与filter进行点乘再相加，形成一个新的feature map里面的一个数字，如果输入的feature map为28 $\times$ 28 $\times$ 1，所产生一个新的feature map只要使用1个filter， 如果输入的feature map是28 $\times$ 28 $\times$ 3，那么产生一个新的feature map则要使用3个filter，</p>
<h2 id="Max-Pooling-池化"><a href="#Max-Pooling-池化" class="headerlink" title="Max Pooling(池化)"></a>Max Pooling(池化)</h2><p><img src="https://gitee.com/heavena888/photo/raw/master/img/psc.jpg" alt=""><br>池化的作用感觉为选出一定区域里面最能表现特征的区域。选择你要进行池化的矩阵大小，在feature map里面选择所对应的大小，并保留最大的数字，循环形成新的feature。</p>
<h2 id="Feedforward-network-全连接层"><a href="#Feedforward-network-全连接层" class="headerlink" title="Feedforward network(全连接层)"></a>Feedforward network(全连接层)</h2><p><img src="https://gitee.com/heavena888/photo/raw/master/img/psc (1" alt="">.jpg)<br>将所训练得到特征进行展开，用DNN的方式最后等到one-hot最后将概率最大的物品，选为图片所对应的物品。</p>
<p><a href="https://poloclub.github.io/cnn-explainer/">呈现像素的cnn实现过程</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>cnn</tag>
      </tags>
  </entry>
  <entry>
    <title>判断过拟合和欠拟合及解决方法</title>
    <url>/2020/08/16/error/</url>
    <content><![CDATA[<h1 id="判断过拟合和欠拟合"><a href="#判断过拟合和欠拟合" class="headerlink" title="判断过拟合和欠拟合"></a>判断过拟合和欠拟合</h1><p>优先判断是tarin data的问题还是test data的问题<br><a id="more"></a><br>如果是test data的问题则使用交叉验证</p>
<h2 id="判断过拟合"><a href="#判断过拟合" class="headerlink" title="判断过拟合"></a>判断过拟合</h2><p>过拟合：个人理解为在test data的时候所训练的模型所能寻找的东西过于具体化，当寻找与其类似的物体时，缺少部分特点就有可能将其划分为别的物体，造成正确率的下降，一般为用交叉验证时，正确率高，但是到了train data里面正确率低。</p>
<h2 id="判断欠拟合"><a href="#判断欠拟合" class="headerlink" title="判断欠拟合"></a>判断欠拟合</h2><p>欠拟合：个人理解为在test train训练得到模型的时候就不能将各种物品的特征找出来，将各种物品混合了，正确率一开始就不对，交叉验证的时候正确率就低了。</p>
<h2 id="通过mean和variance解释过拟合和欠拟合"><a href="#通过mean和variance解释过拟合和欠拟合" class="headerlink" title="通过mean和variance解释过拟合和欠拟合"></a>通过mean和variance解释过拟合和欠拟合</h2><p><img src="https://gitee.com/heavena888/photo/raw/master/img/HL3VMZCN@HG3DNMHT402%MY.png" alt=""><br>low bias and low variance<br>一般这种情况就是我们所需要的模型符合拟合.<br>high bias and low variance<br>这种情况为欠拟合，如图所示加入我们所要寻找的物品为红色的靶心，但是我们所训练出来的模型他所寻找的物品，为图中的那一堆蓝点所形成的原点，所以寻找的物品并不是我们想要的那个物品。<br>low bias and high variance<br>这种情况为过拟合，如图所示所有的蓝点分布在红色的靶心的周围，我们所训练出来的模型有太多的特征了，所以当train data在进行判断的时候，会将train data中的目标物品中部分不含有部分特征的物品判别成别的物品，或则将部分train data含有部分特征的物品判断成目标物品。所以造成真确率下降。</p>
<h1 id="解决test-data或-train-data问题的方法"><a href="#解决test-data或-train-data问题的方法" class="headerlink" title="解决test data或 train data问题的方法"></a>解决test data或 train data问题的方法</h1><p><img src="https://gitee.com/heavena888/photo/raw/master/img/OP0S(ENE~K4~Q@[0A" alt="">BP0WN.png)</p>
<h2 id="解决train-data"><a href="#解决train-data" class="headerlink" title="解决train data"></a>解决train data</h2><h3 id="new-activation-function"><a href="#new-activation-function" class="headerlink" title="new activation function"></a>new activation function</h3><p>会在每一次bp的时候将数值变成其数值的1/4，最后会形成梯度消失的情况,但是想relu就不会。</p>
<h3 id="adaptive-learning-rate"><a href="#adaptive-learning-rate" class="headerlink" title="adaptive learning rate"></a>adaptive learning rate</h3><p>换新的学习速度，这个不知道怎么改，慢慢变小，最后就能到一个局部最优解吧（不知道是不是只有一个局部最优解）。</p>
<h2 id="解决-test-data"><a href="#解决-test-data" class="headerlink" title="解决 test data"></a>解决 test data</h2><h3 id="early-stopping"><a href="#early-stopping" class="headerlink" title="early stopping"></a>early stopping</h3><p><img src="https://gitee.com/heavena888/photo/raw/master/img/QQ图片20200822155854.png" alt=""><br>epochs（所以数据经历一次forward一次bp算一次epochs）<br>如图验证集在经历epochs次数过大的时候total loss将会增加，找到一个最小点然后<br><a href="https://www.datalearner.com/blog/1051537860479157">early stopping 的实现方法</a></p>
<h3 id="dropout-out"><a href="#dropout-out" class="headerlink" title="dropout out"></a>dropout out</h3><p>先选择p%的概率随机去除掉输入层隐藏层每层p%的神经元<br><img src="https://gitee.com/heavena888/photo/raw/master/img/QQ图片20200822162653.png" alt=""><br>然后将删除了神经元的每层的权重乘以1-p%，然后开始训练模型<br><img src="https://gitee.com/heavena888/photo/raw/master/img/QQ图片20200822163340.png" alt=""></p>
<h3 id="regularization"><a href="#regularization" class="headerlink" title="regularization"></a>regularization</h3><p>L-p范数<br>L为矩阵<br>$L_{p}$ = ||X||$_{p}$ = $\sqrt[p]{\sum_{i = 1}^{p}x_{i}^{p}}, X = (x_{1}, x_{2}, ……,x_{n})$<br>L1正则化<br>L1可以增加稀疏性，稀疏性可以理解为将参数更加接近于0，类似我们获得的模型所用的function为这个函数$a<em>x_{1} + b</em>x_{2} + c <em> x_{3} + d </em> x_{1} <em> x_{4}$并且未知数都不等于0，然后用train data进行测试过拟合，然后进行稀疏可能就变成了$a</em>x_{1} + b<em>x_{2} + c </em> x_{3}$可能就拟合了<br><img src="https://gitee.com/heavena888/photo/raw/master/img/QQ图片20200823160557.png" alt=""><br>为什么l1会有稀疏性，因为凸优化算法。（凸优化不会，以后补坑）<br><a href="https://blog.csdn.net/X1009190387/article/details/105522360?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">凸优化算法</a><br>L2正则化(weight decay)<br>L2可以使权重变的更小，有利于数据的拟合。<br><img src="https://gitee.com/heavena888/photo/raw/master/img/QQ图片20200823160619.png" alt=""><br>L1,L2的2维具体图像<br><img src="https://gitee.com/heavena888/photo/raw/master/img/20200823162050.png" alt=""><br>L-p范式与范式球的交点为最优解。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>过拟合和欠拟合</tag>
      </tags>
  </entry>
  <entry>
    <title>k-mean, pca</title>
    <url>/2020/08/24/pca/</url>
    <content><![CDATA[<h1 id="k-mean"><a href="#k-mean" class="headerlink" title="k-mean"></a>k-mean</h1><p><img src="https://gitee.com/heavena888/photo/raw/master/img/QQ图片20200827214422.png" alt=""><br>选择自己要分成几个聚类，然后随机选取几个已有的物品当做初始的点，然后一次判断其他点位到这几个聚点的距离，优先选取靠其最近的聚点，将此物品放置于与其最接近的聚点当中，当所有的物品都放置于对应的聚点的时候，将每个聚点区域里含有的物品推出所有他们的平均值，然后得到新的聚点，再进行上面的操作，直到这次所有聚点的区域中含有的物品与下次所有聚点的区域中含有的物品一样的时候，这时候可以将其认为是收敛的。如果不放心可以多经历几个轮回，当3，4……次数都一样的时候，可以认为其完成收敛。<br><a id="more"></a></p>
<h1 id="pca-主成分分析"><a href="#pca-主成分分析" class="headerlink" title="pca(主成分分析)"></a>pca(主成分分析)</h1><p>pac的操作就是降维，我们想要获得一个新的坐标系，在新的坐标系里面我们所要的是large variance的坐标系<br>$Z = W \cdot X$ W为orthgonal matrix正交矩阵。<br>$W \cdot X$为W点乘X 在向量上的含义为 将X投影到W轴上。</p>
<h2 id="特征值分解"><a href="#特征值分解" class="headerlink" title="特征值分解"></a>特征值分解</h2><p><img src="https://gitee.com/heavena888/photo/raw/master/img/QQ图片20200831180325.png" alt=""><br><img src="https://gitee.com/heavena888/photo/raw/master/img/QQ图片20200831180344.png" alt=""><br>特征值分解使用与方阵<br>var()是方差，cov()是协方差<br>var(x) = E(x - Ex) ^ 2<br>$cov(x_{1}, x_{2}) = E(x_{1} - Ex_{1}) <em> (x_{2} - Ex_{2})$<br>cov(x) = var(x)因为这是一维的<br>我们的目标是找到一个新的正交矩阵也就是一个坐标系，我们要是其满足方差最大的道理，图中使用了拉格朗日乘法数，最后基本就等于是求特征值的方法<br>S</em>w = a*w  S为初始的矩阵，然后a为特征值，w为特征值a在基于s情况下的向量(正交矩阵的一个向量)。</p>
<h2 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h2><p><img src="https://gitee.com/heavena888/photo/raw/master/img/QQ图片20200831232059.png" alt=""><br><img src="https://gitee.com/heavena888/photo/raw/master/img/QQ图片20200831232113.png" alt=""><br><img src="https://gitee.com/heavena888/photo/raw/master/img/QQ图片20200831232140.png" alt=""><br><img src="https://gitee.com/heavena888/photo/raw/master/img/QQ图片20200831232153.png" alt=""><br>$X = U \sum V^{T}$<br>X为M <em> N，U为M </em> K，$\sum$为K <em> K， V为K </em> N<br>U为映射后的单位基向量，V为映射前的单位基向量。<br>$XY = U \sum V^{T} Y$ $\sigma$ = $V^{T}Y$标量<br>我们发现SVD分解$X = U \sum V^{T}$中，左奇异U不就是$XX^{T}$的特征向量。那我们就可以利用SVD分解来计算投影矩阵了.右奇异值V就是$X^{T}X$的特征向量。<br>我们所需要的重要的信息<br><img src="https://gitee.com/heavena888/photo/raw/master/img/v2-f083c7f32411d166ebcbf473b7060889_r.jpg" alt=""><br><a href="https://www.zhihu.com/question/22237507">奇异值的物理意义</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>半监督学习</tag>
      </tags>
  </entry>
</search>
